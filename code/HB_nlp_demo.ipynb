{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are you feeling happy or sad?\n",
    "#### Enter some text, an email, song lyrics, a note from your lover, or just some random thoughts you are mulling on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import requests, time, re\n",
    "import datetime as dt\n",
    "\n",
    "from psaw import PushshiftAPI\n",
    "from textblob import TextBlob\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "from ipywidgets import widgets, interact, interact_manual, fixed\n",
    "from IPython.display import display, clear_output, HTML\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.display.max_columns = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_divider = widgets.Output(layout={'border': '1px solid black'})\n",
    "\n",
    "def on_text_submit(incoming):\n",
    "    showcase = True\n",
    "    display(line_divider)\n",
    "    print(\"Message:\")\n",
    "    print(incoming.value)\n",
    "    new_posts = incoming.value\n",
    "    # Process new post to ascertain classification\n",
    "    new_posts_featurized = reddit_prepare_features(pd.Series(new_posts)) # Featurize new post\n",
    "    new_posts_featurized = reddit_equalize_features(X_features, new_posts_featurized) # Only utilize features actually modeled\n",
    "    predictions = reddit_sample_score(model, new_posts_featurized, showcase=True) # Generate predictions \n",
    "    # Convert predictions to dataframe\n",
    "    df_predictions = pd.DataFrame(columns=['classification', 'post'])\n",
    "    df_predictions['classification'] = predictions\n",
    "    df_predictions['post'] = new_posts\n",
    "    print(\"\\nResult:\")\n",
    "    for e in df_predictions['classification']:\n",
    "        if e == 0: print(\"That message has a sad sentiment to it.\\nConsider watching some furry animal videos while listening to 'Somewhere Over the Rainbow.'\")\n",
    "        else: print(\"That message has a happy sentiment to it.\\nThat makes me happy as well :-)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c608d20e586472ab096a19a3362b3a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display input text box and handle on_submit\n",
    "input_text = widgets.Text()\n",
    "display(input_text)\n",
    "input_text.on_submit(on_text_submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Transform text input to cleaned, corrected, and vectorized feature set as a dataframe\n",
    "#  X: pd.Series of text to prepare into feature set\n",
    "def reddit_prepare_features(X, lowercase=True, max_features=5000):\n",
    "    \n",
    "    # Fix spellings (computing heavy operation)\n",
    "    #df['body'] = df['body'].map(lambda x: str(TextBlob(x).correct()))\n",
    "    \n",
    "    # Transform text to counts of words using CountVectorizer\n",
    "    #  stop_words='english' to remove common English stop words\n",
    "    #  lowercase=True to convert all to lowercase\n",
    "    #  max_features=5000 to limit feature set to 5000 variables\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", token_pattern=r'\\b[^\\d\\W]+\\b', ngram_range=(1, 2), stop_words='english', lowercase=lowercase, max_features=max_features)\n",
    "    X = vectorizer.fit_transform(X)  \n",
    "    X = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    return X\n",
    "\n",
    "# Function: Build, fit, and evaluate model\n",
    "#  model:\n",
    "#  X:\n",
    "#  y:\n",
    "#  Note: the model will not fit if the argument test=True\n",
    "def reddit_model_fit_score(model, X, y):\n",
    "    \n",
    "    # Split dataset into training and testing subsets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "    \n",
    "    # Use model 'lr': LogisticRegression\n",
    "    if model == 'lr':\n",
    "        model_type = 'LogisticRegression'\n",
    "        # Instantiate, fit, and predict\n",
    "        lr = LogisticRegression()\n",
    "        model = lr.fit(X_train, y_train)\n",
    "        predictions = lr.predict(X_train)\n",
    "        # Evaluate model\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        score_cv_train = cross_val_score(lr, X_train, y_train, cv=kf).mean()\n",
    "        score_train = lr.score(X_train, y_train)\n",
    "        score_test = lr.score(X_test, y_test)\n",
    "        #print(\"Using a {0} model we get the following scores:\".format(model_type))\n",
    "        #print(\" Accuracy (cross_validated) on training data: {0:.2f}\".format(score_cv_train))\n",
    "        #print(\" Accuracy on training data: {0:.2f}\".format(score_train))\n",
    "        #print(\" Accuracy on testing data: {0:.2f}\".format(score_test))\n",
    "        #print(\"\")\n",
    "        \n",
    "        \n",
    "    # Use model 'rf': RandomForestClassifier\n",
    "    if model == 'rf':\n",
    "        model_type = 'RandomForestClassifier'\n",
    "        # Instantiate, fit, and predict\n",
    "        rf = RandomForestClassifier()\n",
    "        model = rf.fit(X_train, y_train)\n",
    "        predictions = rf.predict(X_train)\n",
    "        # Evaluate model\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        score_cv_train = cross_val_score(rf, X_train, y_train, cv=kf).mean()\n",
    "        score_train = rf.score(X_train, y_train)\n",
    "        score_test = rf.score(X_test, y_test)\n",
    "        #print(\"Using a {0} model we get the following scores:\".format(model_type))\n",
    "        #print(\" Accuracy (cross_validated) on training data: {0:.2f}\".format(score_cv_train))\n",
    "        #print(\" Accuracy on training data: {0:.2f}\".format(score_train))\n",
    "        #print(\" Accuracy on testing data: {0:.2f}\".format(score_test))\n",
    "\n",
    "    \n",
    "    # Return model used\n",
    "    return model\n",
    "\n",
    "# Function: Equalize testing data columns by imputing missing features with null values. In addition, remove features not in model\n",
    "#  model_features: list of features in model\n",
    "#  new_df: new dataframe\n",
    "def reddit_equalize_features(model_features, new_df):\n",
    "    new_features = new_df.columns.tolist()\n",
    "    if set(model_features) != set(new_features):\n",
    "        missing_cols = set(model_features) - set(new_features)\n",
    "        for col in missing_cols:\n",
    "            new_df[col] = 0\n",
    "    \n",
    "    if set(new_features) != set(model_features):\n",
    "        missing_cols = set(new_features) - set(model_features)\n",
    "        new_df = new_df.drop(missing_cols, axis=1)\n",
    "    \n",
    "    new_df = new_df[model_features]\n",
    "    return new_df\n",
    "\n",
    "# Function: Apply model to sample dataset and generate predictions\n",
    "#  model: model object to use (generated from training dataset)\n",
    "#  X: dataframe of sample features\n",
    "#  showcase: Default=False. If showcase=True, then do not print output \n",
    "def reddit_sample_score(model, X, showcase=False):\n",
    "    predictions = model.predict(X)\n",
    "    if not showcase:\n",
    "        pass\n",
    "        #print(model)\n",
    "        #print(\"\")\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "# Which subreddit does a post belong to?\n",
    "#  Gather data from reddit and using NLP on the title/text classify post as either 1 or 0\n",
    "#  In this case, 1 = happy (from a subreddit called happy), and 0 = sad (from a subreddit called sad)\n",
    "\n",
    "# Read csv files containing 'happy' and 'sad' dataset from reddit\n",
    "df_happy = pd.read_csv('../data/happy_sc_201611_20181219_reddit.csv')\n",
    "df_sad = pd.read_csv('../data/sad_sc_201011_20181219_reddit.csv')\n",
    "\n",
    "# Concatenate datasets\n",
    "df = pd.concat([df_happy, df_sad], ignore_index=True)\n",
    "\n",
    "# Basic dimensions, columns, and datatypes\n",
    "#df.info()\n",
    "\n",
    "# Header of dataset\n",
    "#df.head()\n",
    "\n",
    "# Remove unneeded columns, rows with nulls, and any duplicate posts if present\n",
    "df = df.drop(['Unnamed: 0', 'created', 'created_utc'], axis=1)\n",
    "df = df.drop_duplicates('id')\n",
    "df = df.dropna()\n",
    "\n",
    "# Only include submissions\n",
    "mask = (df['type'] == 'submission')\n",
    "df = df.loc[mask, :]\n",
    "\n",
    "# Insure relative balance of classes\n",
    "#df['subreddit'].value_counts()\n",
    "\n",
    "# Convert target variable 'subreddit' to 1 = happy and 0 = sad\n",
    "df['subreddit'] = df['subreddit'].map(lambda x: 1 if x == 'happy' else 0)\n",
    "\n",
    "# Prepare feature set for model using text vectorization among other type sof pre-processing (see function for details)\n",
    "X = reddit_prepare_features(df['body'], lowercase=True, max_features=10000)\n",
    "X_features = X.columns.tolist()\n",
    "#print(\"Number of features in model: {0:,}\".format(len(X_features)))\n",
    "\n",
    "# Set target variable and features\n",
    "y = df['subreddit']\n",
    "X = X\n",
    "\n",
    "# Fit and score model 'lr': LogisticRegression\n",
    "model_lr = reddit_model_fit_score('lr', X, y)\n",
    "\n",
    "# # Fit and score model 'rf': RandomForestClassifier\n",
    "# model_rf = reddit_model_fit_score('rf', X, y)\n",
    "\n",
    "# Set model to LogisticRegression since more generalized model and with higher accuracy than with a RandomForestClassifier\n",
    "model = model_lr\n",
    "\n",
    "# Input new post and let model ascertain classification\n",
    "new_posts = [\"I am so sad about how things are coming along\",\n",
    "           \"I am so happy about how things are in my life\"]\n",
    "\n",
    "# Process new post to ascertain classification\n",
    "new_posts_featurized = reddit_prepare_features(pd.Series(new_posts)) # Featurize new post\n",
    "new_posts_featurized = reddit_equalize_features(X_features, new_posts_featurized) # Only utilize features actually modeled\n",
    "predictions = reddit_sample_score(model, new_posts_featurized) # Generate predictions\n",
    "\n",
    "# Convert predictions to dataframe\n",
    "df_predictions = pd.DataFrame(columns=['classification', 'post'])\n",
    "df_predictions['classification'] = predictions\n",
    "df_predictions['post'] = new_posts\n",
    "#print(df_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script><a href=\"javascript:code_toggle()\">Toggle</a> code."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script><a href=\"javascript:code_toggle()\">Toggle</a> code.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "7318f1d6b3f7487180632ea50053b6b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "7da25a6b79d0416a8004dc3f92c440a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_bbd7dd74f0b0485a9e89d1d4d1bc18f2",
       "style": "IPY_MODEL_7318f1d6b3f7487180632ea50053b6b6"
      }
     },
     "b46add0847ea44b28ec5d58114cbcbd3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "border": "1px solid black"
      }
     },
     "bbd7dd74f0b0485a9e89d1d4d1bc18f2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f7287a34863e4a48bb92c8f785188134": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_b46add0847ea44b28ec5d58114cbcbd3"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
